[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "James Otto",
    "section": "",
    "text": "I am a doctoral candidate studying Statistics at Baylor University. I have an MS in Statistics as well as undergraduate degrees in Statistics, Mathematics, Economics, and Management of Information Systems.\nI am very interested in computational statistics and have developed several R packages. I also enjoy exploring the connections between theoretical mathematics and statistics—especially applications of algebraic topology and functional analysis.\nWhen not working on research, I’m probably practicing aerial silks, bouldering, or reading."
  },
  {
    "objectID": "shiny.html",
    "href": "shiny.html",
    "title": "James Otto",
    "section": "",
    "text": "Explore the history of the most popular Magic: the Gathering formats. An interactive timeline cataloging major events such as card printings and bannings, accompanied with a plot displaying the monthly metagame breakdown. Includes tournament data from October 2010 onwards, from MTGTOP8.\n\njamesotto.app/MTG-History"
  },
  {
    "objectID": "shiny.html#goodreads-dashboard",
    "href": "shiny.html#goodreads-dashboard",
    "title": "James Otto",
    "section": "Goodreads Dashboard",
    "text": "Goodreads Dashboard\nExplore data from any goodreads.com user profile. See which of the the winners and nominees of various awards you have read (Hugo, Nebula, etc.). Visual summaries of data coming soon!\n\njamesotto.app/Goodreads-Dashboard \nData scraped with goodreadR"
  },
  {
    "objectID": "talks/ggdensity-talk.html",
    "href": "talks/ggdensity-talk.html",
    "title": "ggdensity: Improved Bivariate Density Visualization in R",
    "section": "",
    "text": "Slides (source code)\n\nJSM 2022\nuseR! 2022\nSDSS 2022"
  },
  {
    "objectID": "talks/extending-ggplot2-talk.html",
    "href": "talks/extending-ggplot2-talk.html",
    "title": "Extending ggplot2 with new Geoms and Stats",
    "section": "",
    "text": "Slides (source code)"
  },
  {
    "objectID": "D3.html",
    "href": "D3.html",
    "title": "James Otto",
    "section": "",
    "text": "Magic: the Gathering\nThe plot below visualizes the most popular cards in MTG tournament play over time, looking specifically at legacy events (where decks can be made up of essentially any cards out of the 20,000 that have been printed since 1993). I also have a shiny app which offers a more detailed breakdown of all major formats along with annotated timelines of major events over the last decade.\nThe chart shows the most popular cards during a given month, use the buttons below to navigate by months and years. For each card, there are two values being visualized:\n\n\nPrevalence: the proportion of decks playing at least one copy of the card (bar length and ordering)\n\n\nAverage Copies: the average number of copies of the card played in decks playing at least one copy (bar color)\n\n\nTournament data from MTGTOP8.com, scraped with rvest.\n\n\n\nLegacy metagame breakdown:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n{\n\n  // Using data from R code (below)\n  //  Need to transpose data (column-wise => row-wise)\n  const dataset = transpose(df_mtg);\n  \n  // no. of cards to show in plot\n  const cards = 30;\n  \n  // start at most recent month\n  var t = 0;\n  \n  // return 30 rows of data corresponding to time t\n  var get_dataset_t = function(t) {\n    return dataset.slice((t * cards), (t + 1) * cards);\n  };\n  \n  // convert date to text for header\n  var get_date = function() {\n  \n    const monthNames = [\n      \"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n      \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"\n    ];\n\n    var date = new Date(dataset_window[0].time);\n    \n    return monthNames[date.getMonth()] + \", \" + date.getFullYear();\n    \n  };\n  \n  // accessor for the \"key\" value of our data (the card name)\n  var key = function(d) {\n    return d.card;\n  };\n  \n  // initialize data w/ time t = 0\n  var dataset_window = get_dataset_t(t);\n  \n  \n  \n  // code for drawing card art\n  // uris is object w/ elements = urls for different art formats\n  var update_img = function(uris) {\n  \n    // draw full card in bottom right, fading in quickly\n    svg.select(\"#card-full\")\n      .selectAll(\"image\")\n      .data([uris.png])\n      .enter()\n      .append(\"svg:image\")\n      .attr(\"id\", \"card-full\")\n      .attr(\"xlink:href\", (d) => d)\n      .attr(\"x\", 2 * w/3)\n      .attr(\"y\", h/3 + 30)\n      .attr(\"width\", w/4)\n      .style(\"opacity\", 0)\n      .transition(\"card-full-in\")\n      .duration(200)\n      .ease(d3.easeLinear)\n      .style(\"opacity\", 1);\n       \n    // draw card art behind bars, fading in slower\n    svg.select(\"#card-art\")\n      .selectAll(\"image\")\n      .data([uris.art_crop])\n      .enter()\n      .append(\"svg:image\")\n      .attr(\"clip-path\", \"url(#chart-area)\")\n      .attr(\"id\", \"card-art\")\n      .attr(\"xlink:href\", (d) => d)\n      .attr(\"x\", padding_left)\n      .attr(\"y\", 6) \n      .attr(\"height\", h - padding_bottom)\n      .style(\"opacity\", 0)\n      .transition(\"card-art-in\")\n      .duration(500)\n      .ease(d3.easeLinear)\n      .style(\"opacity\", 0.8); // was .5\n  };\n  \n  // query scryfall api for card art\n  var get_art = function(name) {\n  \n    const reg = /[^\\w\\s]/g\n    var url = name.replace(reg, \"\").replace(\" \", \"+\").toLowerCase();\n    url = \"https://api.scryfall.com/cards/named?fuzzy=\" + url;\n    \n    fetch(url)\n      .then((response) => response.json())\n      // handle multifaced cards, return uris for first face\n      .then((data) => data.image_uris ?? data.card_faces[0].image_uris)\n      .then((uris) => update_img(uris))\n      .catch((error) => {\n        console.log(\"Issue with getting url\");\n    });   \n    \n  };\n  \n  // Remove art before drawing new art\n  // transition is breaking, removed for now\n  var remove_art = function() {\n    svg.select(\"#card-full\")\n      .selectAll(\"image\")\n      .remove();\n    \n    svg.selectAll(\"#card-art\")\n      .selectAll(\"image\")\n//      .transition(\"card-art-out\")\n//      .duration(500)\n//      .ease(d3.easelinear)\n//      .style(\"opacity\", 0)\n      .remove();\n  };\n  \n  var mouseover_fun = function(e, d) {\n    // on mouseover, dim other bars for card art\n    d3.selectAll(\".card-bars\")\n      .attr(\"stroke-width\", 0)\n      .attr(\"opacity\", 0.4);\n  \n    // darken selected bar\n    // (alternatively, could adjust stroke-width)\n    d3.select(this)\n      //.attr(\"stroke-width\", 2)\n      //.attr(\"stroke\", \"white\")\n        .attr(\"opacity\", 1)\n        .attr(\"fill\", \"black\");\n        \n    // finally, draw card art\n    get_art(d.card, update_img);\n  };\n  \n  // on mouseout, return to normal\n  var mouseout_fun = function(d) {\n    d3.selectAll(\".card-bars\")\n      .attr(\"opacity\", 1);\n      \n    d3.select(this)\n        .transition()\n        .duration(200)\n          .attr(\"fill\", (d) => d3.interpolateViridis(cScale(d.average)));\n          \n    remove_art();\n  };\n\n  // Various parameters governing plot dimensions\n  const w = 1200;\n  const h = 800;\n  const padding_left = 150;\n  const padding_right = 35;\n  const padding_bottom = 35;\n  const anim_len = 1500;\n    \n  // x and y scales (simple, linear scales)\n  var xScale = d3.scaleLinear()\n    .domain([0, 1])\n    .range([padding_left, w - padding_right]);\n  \n  var yScale = d3.scaleBand()\n    .domain(d3.range(dataset_window.length))\n    .rangeRound([0, h - padding_bottom])\n    .paddingInner(0.075);\n    \n  // color scale, with compse w/ viridis\n  var cScale = d3.scaleLinear()\n    .domain([0, 4]);\n  \n  // the main svg we will draw on\n  var svg = d3.select(\".plot-mtg\")\n    .append(\"svg\")\n    .attr(\"preserveAspectRatio\", \"xMinYMin meet\")\n    .attr(\"viewBox\", \"0 0\" + \" \" + w + \" \" + h);\n    \n    \n  // Now, need to set up a few groups\n  //  (this allows for use to draw art under the bars)\n    \n  // Group for axis needs to be first\n  svg.append(\"g\").attr(\"class\", \"axis\");\n    \n  // set up groups for card art types\n  svg.append(\"g\").attr(\"id\", \"card-art\");\n  svg.append(\"g\").attr(\"id\", \"card-full\");\n    \n  // Need clipping path (mask) for bottom edge\n  //  (Prevents bars + text going below axis)\n  //  (Also keeps card art in the right spot)\n  svg.append(\"clipPath\")\n    .attr(\"id\", \"chart-area\")\n    .append(\"rect\")\n    .attr(\"x\", 0)\n    .attr(\"y\", 0)\n    .attr(\"width\", w - padding_right)\n    .attr(\"height\", h - padding_bottom);\n    \n  // Draw initial bars \n  //  if we want stroke-width on mouseover, need to initialize here\n  svg.selectAll(\".card-bars\")\n    .data(dataset_window, key)\n    .enter()\n    .append(\"rect\")\n    .attr(\"class\", \"card-bars\")\n    .attr(\"clip-path\", \"url(#chart-area)\")\n    .attr(\"x\", (d) => xScale(0))\n    .attr(\"width\", (d) => xScale(d.prevalence) - xScale(0))\n    .attr(\"y\", (d, i) => yScale(i))\n    .attr(\"height\", yScale.bandwidth())\n    .attr(\"fill\", (d) => d3.interpolateViridis(cScale(d.average)))\n//    .attr(\"stroke-width\", \"2\")\n//    .attr(\"stroke\", \"white\")\n    .on(\"mouseover\", mouseover_fun)\n    .on(\"mouseout\", mouseout_fun);\n    \n  // Initial titles:\n  svg.selectAll(\".card-titles\")\n    .data(dataset_window, key)\n    .enter()\n    .append(\"text\")\n    .text((d) => d.card + \"  \")\n    .attr(\"class\", \"card-titles\")\n    .attr(\"clip-path\", \"url(#chart-area)\")\n    .attr(\"font-size\", \"13px\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"style\",\"white-space:pre\")\n    .attr(\"x\", (d) => xScale(0))\n    .attr(\"y\", (d, i) => yScale(i) + yScale.bandwidth() * 5/8);\n\n      \n  // x-axis with formatting:\n  var xAxis = d3.axisBottom()\n    .scale(xScale)\n    .ticks(4)\n    .tickFormat(d3.format(\".0%\"));\n  \n  svg.select(\".axis\")\n    .attr(\"transform\", \"translate(0,\" + (h - padding_bottom) + \")\")\n    .call(xAxis);\n  \n  // Note: there is no y-axis\n  // instead, we manually handled the bar labels as \"text\" objects\n  // this allows for pretty transitions\n        \n  // UI element set up:\n  // Set \">\" button as disabled on start up:\n  d3.select(\"#next\").property(\"disabled\", true);\n  d3.select(\"#nnext\").property(\"disabled\", true);\n    \n  // Set time in header\n  d3.select(\"#time\")\n    .append(\"text\")\n    .text(get_date());\n    \n  // When a button is pressed, start here\n  d3.selectAll(\"button\")\n    .on(\"click\", function() {\n     \n      // See which button was clicked\n      var buttonID = d3.select(this).attr(\"id\");\n            \n            // logic to prevent going past bounds of data\n            if (buttonID == \"prev\") {\n              if (t < 100) {\n                t = t + 1;\n        };\n            };\n            \n          if (buttonID == \"pprev\") {\n              if (t <= 88) {\n                 t = t + 12;\n         }; \n            };\n            \n          if (buttonID == \"next\") {\n              if (t > 0) {\n                t = t - 1;\n        };\n            };\n            \n            if (buttonID == \"nnext\") {\n              if (t >= 12) {\n                t = t - 12;\n        };\n            };\n            \n            // reset data with new value of t\n            dataset_window = get_dataset_t(t);\n            \n            // re-bind new data to bars\n      var bars = svg.selectAll(\".card-bars\")\n        .data(dataset_window, key);\n        \n      // redraw bars\n      bars.enter()\n        .append(\"rect\")\n        .attr(\"class\", \"card-bars\")\n        .attr(\"clip-path\", \"url(#chart-area)\")\n        .attr(\"x\", (d) => xScale(0))\n                .attr(\"y\", (d, i) => yScale(i) + h) // start with y value below axis\n        .attr(\"height\", yScale.bandwidth())\n        .on(\"mouseover\", mouseover_fun)\n        .on(\"mouseout\", mouseout_fun)\n        .merge(bars)    // Now looking at ALL bars\n                .transition(\"bars-enter\")\n                .duration(anim_len)\n        .attr(\"width\", (d) => xScale(d.prevalence) - xScale(0))\n        .attr(\"y\", (d, i) => yScale(i)) // update y value to be correct\n        .attr(\"fill\", (d) => d3.interpolateViridis(cScale(d.average)));\n\n      // remove old bars\n            bars.exit()\n                .transition(\"bars-exit\")\n                .duration(anim_len)\n                .attr(\"y\", (d, i) => yScale(i) + h) // travel out of window\n                .remove();\n                \n                \n            // similar to bars, but now with text:\n        var labs = svg.selectAll(\".card-titles\")\n        .data(dataset_window, key);\n        \n      labs.enter()\n        .append(\"text\")\n        .text((d) => d.card + \"  \")\n        .attr(\"class\", \"card-titles\")\n        .attr(\"clip-path\", \"url(#chart-area)\")\n        .attr(\"font-size\", \"13px\")\n        .attr(\"text-anchor\", \"end\")\n        .attr(\"style\",\"white-space:pre\")\n        .attr(\"x\", (d) => xScale(0))\n                .attr(\"y\", (d, i) => yScale(i) + h)\n        .merge(labs)    // Now looking at ALL text\n                .transition(\"labs-enter\")\n                .duration(anim_len)\n        .attr(\"y\", (d, i) => yScale(i) + yScale.bandwidth() * 5/8);\n        \n       labs.exit()\n               .transition(\"labs-exit\")\n                 .duration(anim_len)\n                 .attr(\"y\", (d, i) => yScale(i) + h)\n                 .remove();\n                \n           // update header with correct time:\n       d3.select(\"#time\")\n         .select(\"text\")\n         .remove();\n         \n       // Disable/enable buttons as necessary:\n       d3.select(\"#next\").property(\"disabled\", t == 0);\n       d3.select(\"#nnext\").property(\"disabled\", t < 12);\n       \n       d3.select(\"#prev\").property(\"disabled\", t == 100);\n       d3.select(\"#pprev\").property(\"disabled\", t > 88);\n         \n       // update month + year\n       d3.select(\"#time\")\n         .append(\"text\")\n         .text(get_date());\n      });\n\n}\n\n\n\n\n\n\n\n\n\nCode\n{\n  // draw legend below plot svg\n  // adapted from https://observablehq.com/@tmcw/d3-scalesequential-continuous-color-legend-example\n  const w = 1200;\n  const h = 50\n  const barHeight = 25\n  \n  const margin = ({top: 0, right: 35, bottom: 25, left: 150})\n\n  var colorScale = d3.scaleSequential(d3.interpolateViridis).domain([0, 4])\n  \n  var axisScale = d3.scaleLinear()\n    .domain(colorScale.domain())\n    .range([margin.left, w - margin.right])\n    \n  var axisBottom = g => g\n    .attr(\"class\", \"axisWhite\")\n    .attr(\"transform\", `translate(0,${h - margin.bottom})`)\n    .call(d3.axisBottom(axisScale)\n      .ticks(5)\n      .tickSize(-barHeight))\n      \n  var svg_legend = d3.select(\".legend\")\n    .append(\"svg\")\n    .attr(\"preserveAspectRatio\", \"xMinYMin meet\")\n    .attr(\"viewBox\", \"0 0\" + \" \" + w + \" \" + h);\n    \n  var defs = svg_legend.append(\"defs\");\n  \n  var linearGradient = defs.append(\"linearGradient\")\n      .attr(\"id\", \"linear-gradient\");\n  \n  linearGradient.selectAll(\"stop\")\n    .data(colorScale.ticks().map((t, i, n) => ({ offset: `${100*i/n.length}%`, color: colorScale(t) })))\n    .enter().append(\"stop\")\n    .attr(\"offset\", d => d.offset)\n    .attr(\"stop-color\", d => d.color);\n  \n  svg_legend.append('g')\n      .attr(\"transform\", `translate(0,${h - margin.bottom - barHeight})`)\n      .append(\"rect\")\n      .attr('transform', `translate(${margin.left}, 0)`)\n    .attr(\"width\", w - margin.right - margin.left)\n    .attr(\"height\", barHeight)\n    .style(\"fill\", \"url(#linear-gradient)\");\n    \n  svg_legend.append('g')\n    .call(axisBottom);  \n  \n  // Color guide title\n  svg_legend.append(\"text\")\n    .text(\"Avg no. of Copies:  \")\n    .attr(\"font-size\", \"13px\")\n    .attr(\"text-anchor\", \"end\")\n    .attr(\"style\",\"white-space:pre\")\n    .attr(\"x\", margin.left)\n        .attr(\"y\", h/3)\n}\n\n\n\n\n\n\n\n\n\nR Code (Data Wrangling)\nlibrary(\"tidyverse\")\n# Reading in data\n# Data is scraped from MTGTOP8, details coming soon in a blog post\ndf_mtg <- read_csv(here::here(\"posts/2022-11-25-JavaScript-and-Quarto/data/legacy.csv\")) \n\n# Helper to fix encoding of dates\n## 2000.05 => 2000-01-01\n## 2004.25 => 2000-04-01\nfix_time <- function(t) {\n  \n  year <- floor(t)\n  month <- round(20 * (t - year))\n  \n  lubridate::ymd(paste(year, month, \"1\"))\n  \n}\n\n# We don't want basic lands in our viz\nbasics <- c(\"Plains\", \"Mountain\", \"Forest\", \"Island\", \"Swamp\")\n\ndf_mtg <-\n  df_mtg |>\n  filter(!is.na(time)) |>\n  # Only want to look at top-8 places\n  filter(place %in% c(1, 2, 5, 8)) |> \n  mutate(time = fix_time(time)) |>\n  filter(lubridate::year(time) >= 2011) |>\n  # Don't want cards from sideboard \n  filter(!SB) |>\n  filter(!card %in% basics) |>\n  # Find count of each card at each timepoint, regardless of place (data is grouped by place)\n  group_by(time, card) |>\n  summarize(k = n(), copies = sum(copies), decks = sum(decks), total_decks = sum(total_decks), .groups = \"drop_last\") |> \n  mutate(prevalence = decks / total_decks, average = copies / decks) |>\n  # Randomized pertubation to avoid ties return > 30 rows\n  top_n(30, wt = (prevalence + rnorm(length(prevalence), sd = .000001))) |> \n  # Break ties w/ average\n  arrange(desc(time), desc(prevalence), average) |>\n  # For extensibiliity\n  mutate(format = \"legacy\")\n\n# Make `df_mtg` available in ojs chunks:\nojs_define(df_mtg = df_mtg)"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "James Otto",
    "section": "",
    "text": "Data Visualization\n\n\n\nggdensity: Improved density visualization in ggplot2\n\n\nTDAvis: Visualizing tools from topological data analysis\n\n\nggspatreg: Plotting spatial regression model predictions\n\n\nggrrr: Functional programming with ggplot2 and patchwork\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocumentation\n\n\n\ntldr: Short-form documentation in the R console\n\n\ntldrDocs: tldr documentation for base R objects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMiscellaneous\n\n\n\ngoodreadR: Scrape user data from goodreads.com\n\n\nfRiend: An octopus greeting in each new R session\n\n\n\n\n\n\n\n \n\n\n\n\n\nData Visualization\n\n\n\nggdensity: Improved density visualization in ggplot2\n\n\nTDAvis: Visualizing tools from topological data analysis\n\n\nggspatreg: Plotting spatial regression model predictions\n\n\nggrrr: Functional programming with ggplot2 and patchwork\n\n\n\n\n\nDocumentation\n\n\n\ntldr: Short-form documentation in the R console\n\n\ntldrDocs: tldr documentation for base R objects\n\n\n\n\n\nMiscellaneous\n\n\n\ngoodreadR: Scrape user data from goodreads.com\n\n\nfRiend: An octopus greeting in each new R session"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Extending ggplot2 with new Geoms and Stats\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nJames Otto\n\n\n\n\n\n\n  \n\n\n\n\nggdensity: Improved Bivariate Density Visualization in R\n\n\n\n\n\n\n\n\n\nAug 7, 2022\n\n\nJames Otto, David Kahle\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html",
    "href": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html",
    "title": "Sentiment towards The Rings of Power",
    "section": "",
    "text": "The Rings of Power is a controversial show that fills in the time before the events in J. R. R. Tolkien’s The Lord of the Rings with what some say is creative reimagining and others decry as glorified fan fiction. The root cause of this debate is that the show’s creators do not have the rights to all of the relevant source material. As a result they have had to go against what is technically canon, playing loose with timelines and at times making up major plot points.\nIts main subreddit, /r/RingsofPower, has two weekly discussion threads per episode—one for book readers and one for non-book readers. Now that the first season has wrapped up, these threads represent a cool statistical opportunity: we can try to compare the feelings and reactions of these two groups across each episode!\nNow, a warning: this post analyzes and visualizes text data from reddit.com. This includes text with coarse language that some may find offensive—if that is you, maybe skip this post.\nNow, with that out of the way, let’s load all of the packages we’ll be needing:"
  },
  {
    "objectID": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html#a-brief-introduction-to-the-reddit-api",
    "href": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html#a-brief-introduction-to-the-reddit-api",
    "title": "Sentiment towards The Rings of Power",
    "section": "A brief introduction to the Reddit API",
    "text": "A brief introduction to the Reddit API\nUsing tools from httr and jsonlite we can easily query the Reddit API (documented here) . Here, we get the 50 highest voted top-level comments from this thread on Episode 7.\n\nres <- \n  GET(\n    \"https://www.reddit.com/r/RingsofPower/comments/xxoyqz.json?sort=top&depth=1&limit=50\",\n    add_headers(\"user-agent\" = \"Rings-of-Power-Scraping\")\n  )\n\ndata <- fromJSON(rawToChar(res$content))\ndata <- data$data$children[[2]]$data\n\nAfter some manipulation, data is now a data.frame with one row per comment and lots of columns, most of which are irrelevant to this analysis. The columns we care about are data$score (each comment’s votes) and data$body (the textual content of each comment):\n\ndata$score\n\n [1] 165 142  93  91  87  84  81  70  64  66  60  55  54  52  52  42  49  44  41\n[20]  39  40  36  38  37  36  42  32  39  35  32  30  31  32  32  29  28  30  29\n[39]  28  26  27  30  28  25  27  24  23  21  23  19  NA\n\ndata$body[1:3]\n\n[1] \"That speech about Harfoots staying true to each other was hilarious after they left him and his family for dead over a sprained ankle.\"                                            \n[2] \"What is with the PowerPoint Mordor text?!\\nYOU HAVE A GOOD ACTOR RIGHT THERE TO SAY MORDOR?\\nThen all the orc boys and gals coulda been like MORDOR MORDOR MORDOR\"                 \n[3] \"I was holding out hope that the “mithril will save the elves” plot line was a misdirect and the result of Annatar trying to deceive Gil-Galad. \\n\\nNope. Mithril is magical. \\n\\nK\"\n\n\nThis looks promising, the scores are decreasing and the text looks like reddit comments! Cross-referencing with the previously linked thread we can see that everything is working as intended, now it’s time to scale this example up."
  },
  {
    "objectID": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html#all-the-data",
    "href": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html#all-the-data",
    "title": "Sentiment towards The Rings of Power",
    "section": "All the data",
    "text": "All the data\nThis step is considerably more complicated, we need to get the 50 highest voted top-level comments for each discussion thread and then use tools from tidytext to “tokenize” each comment and identify words with “positive” and “negative” sentiments.\nFor now on, I’m going to collapse the code for manipulating data and generating graphics. If you are interested I encourage you to look through how I’ve done everything, but know that this post is not a guide on how to perform sentiment analysis. I highly recommend Text Mining with R: A Tidy Approach by Julia Silge and David Robinson if you would like to learn about how to use these tools.\nThe plot below shows the 20 most frequently occurring positive and negative words from the combined threads, according to the Bing lexicon:\n\n\nCode\ndf_urls <- \n  tibble(\n    episode = rep(2:8, times = 2),\n    book = rep(c(TRUE, FALSE), each = 7),\n    url = c(\n      \n      # Book Spoiler threads:\n      \"x3qfr2\", # Ep 1, 2\n      \"x9ngql\", # Ep 3\n      \"xfgxa1\", # Ep 4\n      \"xlmuu5\", # Ep 5\n      \"xrrbrm\", # Ep 6\n      \"xxoyqz\", # Ep 7\n      \"y3j1zg\", # Ep 8\n      \n      # Non-Spoiler threads:\n      \"x3qfqz\", # Ep 1, 2\n      \"x9ngqa\", # Ep 3\n      \"xfgx9y\", # Ep 4\n      \"xlmurh\", # Ep 5\n      \"xrrbtm\", # Ep 6\n      \"xxoyvo\", # Ep 7\n      \"y3j23u\"  # Ep 8\n      \n    )\n  ) |>\n  mutate(\n    url = glue(\"https://www.reddit.com/r/RingsofPower/comments/{url}.json?sort=top&depth=1&limit=50\")\n  )\n\ncomments_as_df <- function(url) {\n  \n  # Need to set user-agent header to avoid 429 error\n  res <- GET(url, add_headers(\"user-agent\" = \"Rings-of-Power-Scraping\"))\n  data <- fromJSON(rawToChar(res$content))\n  data <- data$data$children[[2]]$data \n  \n  data |>\n    select(body, score) |>\n    mutate(comment_id = 1:n())\n  \n}\n\ndf_urls <- df_urls |>\n  rowwise() |>\n  mutate(\n    comments = list(comments_as_df(url)),\n  )\n\n# For each element (data.frame) in df_urls$comments,\n# we're tokenizing with unnest_tokens() and removing stop words:\n# df_urls$comments[[1]] |>\n#   unnest_tokens(word, body) |>\n#   anti_join(stop_words)\n\ndf_urls <- df_urls |>\n  mutate(\n    tidy_body = list(unnest_tokens(comments, word, body)),\n    tidy_body = list(anti_join(tidy_body, stop_words))\n  )\n\ndf_words <- df_urls |>\n  unnest(tidy_body) |>\n  select(episode, book, comment_id, word) |>\n  inner_join(get_sentiments(\"bing\")) \n\ndf_words |>\n  count(word, sentiment, sort = TRUE) |>\n  group_by(sentiment) |>\n  slice_max(n, n = 20) |>\n  mutate(word = reorder(word, n)) |>\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(vars(sentiment), ncol = 1, scales = \"free_y\") +\n  labs(\n    y = NULL,\n    x = \"Instances\"\n  )\n\n\n\n\n\n\n\n\nA few thoughts on the above plot. First, we see that Redditors do indeed love their four letter words—this is no surprise. Second (and more important), it looks like this naive attempt at sentiment analysis is getting hung up on words with particular meanings in the context of The Rings of Power. For example, 3 out of 5 of the most common “negative” words (\"plot\", \"stranger\", \"evil\") are obviously not conveying a negative sentiment, each with a specific neutral meaning in fantasy in general or in relation to the happenings in the show. Let’s remove a few of the obvious false negatives/positives and see how the resulting plot looks:\n\n\nCode\ndf_words <- df_words |>\n  filter(\n    ! word %in% c(\n      # False-Negatives:\n      \"plot\", \"stranger\", \"evil\", \"dead\", \"death\", \"doom\", \"die\",\n      \"kill\", \"died\", \"conflict\", \"tension\", \"corrupted\", \"forged\",\n      # False-Positives:\n      \"magic\", \"powerful\"\n    )\n  )\n\ndf_words |>\n  count(word, sentiment, sort = TRUE) |>\n  group_by(sentiment) |>\n  slice_max(n, n = 20) |>\n  mutate(word = reorder(word, n)) |>\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(vars(sentiment), ncol = 1, scales = \"free_y\") +\n  labs(\n    y = NULL,\n    x = \"Instances\"\n  )\n\n\n\n\n\nMuch better! Now that we feel more confident about the words that we’ve identified as positive and negative, let’s proceed with a more in-depth analysis."
  },
  {
    "objectID": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html#analyzing-trends",
    "href": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html#analyzing-trends",
    "title": "Sentiment towards The Rings of Power",
    "section": "Analyzing trends",
    "text": "Analyzing trends\nAll that’s left for us to do is to compare the average sentiment between the book and non-book discussions. To do that, we’ll assign a score of +1 to positive words and -1 to negative words and compute the “average sentiment” for comments in each thread:\n\n\nCode\ndf_sentiments <- df_words |>\n  count(episode, book, comment_id, sentiment) |>\n  group_by(episode, book) |>\n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |>\n  mutate(sentiment = positive - negative) |>\n  summarize(avg_sentiment = mean(sentiment))\n\nggplot(df_sentiments, aes(episode, avg_sentiment, color = book, group = book)) +\n  geom_abline(slope = 0, intercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_path() +\n  scale_color_manual(NULL, values = c(\"slateblue\", \"firebrick\"), labels = c(\"Non-Book Readers       \", \"Book Readers\")) +\n  scale_x_continuous(breaks = 2:8, labels = c(\"1 + 2\", 3:8)) +\n  theme(legend.position = \"top\")\n\n\n\n\n\nInteresting! It looks like non-book readers were initially positive about the show but slowly and consistently trended to be more negative over time, whereas book readers have been consistently negative. Both groups seem to have experienced an increase in average sentiment with the finale, an effect that is more pronounced in the case of the non-book readers. This generally agrees with my anecdotal experiences from talking with fellow Tolkien fans, however I expected the sentiment around the finale to be significantly more negative.\nAdding some formatting, we can get a more visually interesting graphic:\n\n\nCode\nfont_add_google(\"MedievalSharp\")\nfont_add_google(\"Roboto Condensed\")\n\nshowtext_auto()\n\n# Save w/ width = 1600px and height = 900px\nggplot(df_sentiments, aes(episode, avg_sentiment, color = book, group = book)) +\n  geom_abline(slope = 0, intercept = 0) +\n  geom_point(size = 2.5, show.legend = FALSE) +\n  geom_path(size = 1.5, key_glyph = draw_key_timeseries) +\n  scale_color_manual(NULL, values = c(\"#376f52\", \"#47637e\"), labels = c(\"Non-Book Readers       \", \"Book Readers\")) +\n  scale_x_continuous(breaks = 2:8, labels = c(\"1 & 2\", 3:8)) +\n  labs(\n    x = \"Episode\",\n    y = \"Average Sentiment\",\n    caption = \"Data from /r/RingsofPower \\n Created by @jamesotto852\"\n  ) +\n  theme(\n    text = element_text(\"MedievalSharp\", colour = \"#534137\"),\n    plot.caption = element_text(\"Roboto Condensed\", size = 15),\n    axis.title = element_text(size = 21),\n    axis.text = element_text(size = 15),\n    axis.title.y = element_text(margin = margin(r = 10)),\n    legend.text = element_text(size = 18),\n    legend.position = \"top\",\n    legend.key.size = unit(.4, \"cm\"),\n    panel.grid.major.y = element_line(colour = alpha(\"black\", .1)),\n    panel.grid.major.x = element_blank(),\n    plot.background = element_rect(fill = \"#ffe791\"),\n    plot.margin = unit(rep(.8, 4), \"cm\")\n  )"
  },
  {
    "objectID": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html#final-thoughts",
    "href": "posts/2022-10-15-Rings-of-Power/Rings-of-Power.html#final-thoughts",
    "title": "Sentiment towards The Rings of Power",
    "section": "Final thoughts",
    "text": "Final thoughts\nWhile the results we found are interesting it’s important to remember that this is a very simple analysis based on positive and negative associations of individual words. While we did our best to remove words that were obviously being misclassified, I am sure that that there are still words that are being incorrectly identified as positive or negative that are biasing our results.\nA more involved analysis is certainly called for; for example it would be good to consider larger “n-grams”, allowing our tokens to consist of sequences of words. This approach would be more robust and would likely result in more interesting and nuanced conclusions.\nIf you are interested in analyzing this data on your own,  I have made the data used in this analysis available for download. If you find anything please let me know!\n\nBonus: some word clouds!\nggwordcloud is a really powerful package which provides robust tools for making word clouds within the ggplot2 framework. Below, we include a word cloud with the 20 most popular positive and negative words, sized according to their relative frequency:\n\n\nCode\ndf_words |>\n  count(word, sentiment, sort = TRUE) |>\n  group_by(sentiment) |>\n  slice_max(n, n = 20) |>\n  ungroup() |>\n  arrange(desc(n)) |>\n  ggplot(aes(label = word, size = n, color = sentiment)) +\n  geom_text_wordcloud(family = \"MedievalSharp\") +\n  geom_text_wordcloud() +\n  scale_radius(range = c(0, 40), limits = c(0, NA)) +\n  scale_color_manual(values = c(\"#f4142e\", \"#2b5a4a\")) +\n  theme_void() +\n  theme(\n    panel.background = element_rect(fill = \"#ffe791\")\n  )\n\n\n\nAnd now, in everyone’s favorite secret shape:\n\n\nCode\nmask <- png::readPNG(here::here(\"posts/2022-10-15-Rings-of-Power/Sauron Symbol.png\"))\n\n# Save with width = 2000px, height = 1355px\ndf_words |>\n  count(word, sentiment, sort = TRUE) |>\n  group_by(sentiment) |>\n  slice_max(n, n = 35) |>\n  ungroup() |>\n  slice_sample(prop = 1) |>\n  mutate(angle = 45 * rnorm(n(), sd = .5)) |>\n  ggplot(aes(label = word, size = n, color = sentiment, angle = angle)) +\n  geom_text_wordcloud(family = \"MedievalSharp\", mask = mask, eccentricity = .35) +\n  scale_radius(range = c(2, 17), limits = c(1, NA)) +\n  scale_color_manual(values = c(\"#f4142e\", \"#2b5a4a\")) +\n  theme_void() +\n  theme(\n    panel.background = element_rect(fill = \"#ffe791\")\n  )"
  },
  {
    "objectID": "posts/2022-01-24-Understanding-base-documentation-functions/Understanding-base-documentation-functions.html",
    "href": "posts/2022-01-24-Understanding-base-documentation-functions/Understanding-base-documentation-functions.html",
    "title": "Understanding base documentation functions",
    "section": "",
    "text": "I am working on a project dealing with documentation in R and recently did a deep-dive into how ? and help() work. This post summarizes what I’ve learned about these functions, first briefly discussing how they “work” in a general sense, then going through their implementations line-by-line to understand the functions at a low-level."
  },
  {
    "objectID": "posts/2022-01-24-Understanding-base-documentation-functions/Understanding-base-documentation-functions.html#how-they-work",
    "href": "posts/2022-01-24-Understanding-base-documentation-functions/Understanding-base-documentation-functions.html#how-they-work",
    "title": "Understanding base documentation functions",
    "section": "How they “work”",
    "text": "How they “work”\nThe ? operator is just a convenience function, allowing users to retrieve documentation on objects specified in a variety of ways. Below I’ve included a few examples which all do the same thing, showcasing how flexible ? is:\n\n?anova\n?anova()\n?anova(lm(speed ~ dist, cars))\n?anova(stop())\n?\"anova\"\n?stats::anova\n\nIt achieves this flexibility by using functions like substitute() and eval() to parse its input, eventually leading to a call to help(), help.search(), .helpForCall(), or .tryHelp().\nThese are all very similar, and it is sufficient to look into the help() function to understand what is going on. The main way help() works is by using the functions loadedNamespaces(), find.package(), and utils:::index.search() to find the relevant package files and documentation. Again help() has been implemented to be very flexible, accepting arguments in a variety of forms."
  },
  {
    "objectID": "posts/2022-01-24-Understanding-base-documentation-functions/Understanding-base-documentation-functions.html#how-they-work-the-nitty-gritty",
    "href": "posts/2022-01-24-Understanding-base-documentation-functions/Understanding-base-documentation-functions.html#how-they-work-the-nitty-gritty",
    "title": "Understanding base documentation functions",
    "section": "How they work: the nitty-gritty",
    "text": "How they work: the nitty-gritty\n\n?\nFirst, let’s look at the definition of ?:\n\n`?`\n\nfunction (e1, e2) \n{\n    if (missing(e2)) {\n        type <- NULL\n        topicExpr <- substitute(e1)\n    }\n    else {\n        type <- substitute(e1)\n        topicExpr <- substitute(e2)\n    }\n    search <- (is.call(topicExpr) && topicExpr[[1L]] == \"?\")\n    if (search) {\n        topicExpr <- topicExpr[[2L]]\n        if (is.call(te <- topicExpr) && te[[1L]] == \"?\" && is.call(te <- topicExpr[[2L]]) && \n            te[[1L]] == \"?\") {\n            cat(\"Contacting Delphi...\")\n            flush.console()\n            Sys.sleep(2 + stats::rpois(1, 2))\n            cat(\"the oracle is unavailable.\\nWe apologize for any inconvenience.\\n\")\n            return(invisible())\n        }\n    }\n    if (is.call(topicExpr) && (topicExpr[[1L]] == \"::\" || topicExpr[[1L]] == \n        \":::\")) {\n        package <- as.character(topicExpr[[2L]])\n        topicExpr <- topicExpr[[3L]]\n    }\n    else package <- NULL\n    if (search) {\n        if (is.null(type)) \n            return(eval(substitute(help.search(TOPIC, package = PACKAGE), \n                list(TOPIC = as.character(topicExpr), PACKAGE = package))))\n        else return(eval(substitute(help.search(TOPIC, fields = FIELD, \n            package = PACKAGE), list(TOPIC = as.character(topicExpr), \n            FIELD = as.character(type), PACKAGE = package))))\n    }\n    else {\n        if (is.null(type)) {\n            if (is.call(topicExpr)) \n                return(.helpForCall(topicExpr, parent.frame()))\n            topic <- if (is.name(topicExpr)) \n                as.character(topicExpr)\n            else e1\n            return(eval(substitute(help(TOPIC, package = PACKAGE), \n                list(TOPIC = topic, PACKAGE = package))))\n        }\n        else {\n            type <- if (is.name(type)) \n                as.character(type)\n            else e1\n            topic <- if (is.name(topicExpr)) \n                as.character(topicExpr)\n            else {\n                if (is.call(topicExpr) && identical(type, \"method\")) \n                  return(.helpForCall(topicExpr, parent.frame(), \n                    FALSE))\n                e2\n            }\n            if (type == \"package\") \n                package <- topic\n            h <- .tryHelp(topicName(type, topic), package = package)\n            if (is.null(h)) {\n                if (is.language(topicExpr)) \n                  topicExpr <- deparse(topicExpr)\n                stop(gettextf(\"no documentation of type %s and topic %s (or error in processing help)\", \n                  sQuote(type), sQuote(topicExpr)), domain = NA)\n            }\n            h\n        }\n    }\n}\n<bytecode: 0x55cf73a1f258>\n<environment: namespace:utils>\n\n\nThat’s a big wall of code. We’re going to go through the definition in chunks to better understand what is going on.\n\nImmediately, there’s something I didn’t know before: ? is a function of two arguments! After reading through the documentation, it looks like the optional second argument, e2, allows for documentation of S4 methods.  For most use-cases, e2 will never be specified and the if (missing(e2)) condition will always evaluate to TRUE. This means type will almost always be NULL, and topicExpr will always be e1 (the only argument supplied to ?).\n\n\nfunction (e1, e2)\n{\n    if (missing(e2)) {\n        type <- NULL\n        topicExpr <- substitute(e1)\n    }\n    else {\n        type <- substitute(e1)\n        topicExpr <- substitute(e2)\n    }\n\n\nAbove, note the use of substitute(). Advanced R covers how substitute() is used for quoting. This is exactly what’s going on here—it is being used to capture the unevaluated argument, e1, assigning it to the variable topicExpr (technically, substitute() returns a “parse tree”). The rest of the code is going to deal with picking apart topicExpr to determine what documentation to serve up.\n\nNext up, we’re defining a Boolean variable: search:\n\n\n    search <- (is.call(topicExpr) && topicExpr[[1L]] == \"?\")\n\n\nSee that search is TRUE whenever 1.) topicExpr is an unevaluated function call and 2.) the first element of the parse tree returned by substitute() is the function ?. Why does this matter? It turns out, ?? is not a function— it is the composition of two ? operators! For example: when you run ??tibble you are actually executing `?`(?tibble).\nSo, search is TRUE when the double question mark has been used. This makes sense, as ?? is used for a more general search of the documentation (using the help.search() function, as we’ll see later).\n\nNext up is an Easter Egg. Yup, an Easter Egg. But first, if search is TRUE, we remove the additional ? function (the first element of topicExpr) on line 13:\n\n\n    if (search) {\n        topicExpr <- topicExpr[[2L]]\n        if (is.call(te <- topicExpr) && te[[1L]] == \"?\" && is.call(te <- topicExpr[[2L]]) && \n            te[[1L]] == \"?\") {\n            cat(\"Contacting Delphi...\")\n            flush.console()\n            Sys.sleep(2 + stats::rpois(1, 2))\n            cat(\"the oracle is unavailable.\\nWe apologize for any inconvenience.\\n\")\n            return(invisible())\n        }\n    }\n\n\nNow, the Easter Egg. The conditional on lines 14-15 evaluates to TRUE if there were four nested ? functions. In this case, a message about the Oracle of Delphi is printed at the console.\n\n????sum\n## Contacting Delphi...the oracle is unavailable.\n## We apologize for any inconvenience.\n\n\nMoving on, we now deal with the double and triple colon operators (:: and :::):\n\n\n    if (is.call(topicExpr) && (topicExpr[[1L]] == \"::\" || topicExpr[[1L]] == \n        \":::\")) {\n        package <- as.character(topicExpr[[2L]])\n        topicExpr <- topicExpr[[3L]]\n    }\n    else package <- NULL\n\n\nIn the presence of these operators, we assign the relavent package to the package variable and the function to the topicExpr variable. Below, we have included an example of how this works:\n\ntopicExpr <- substitute(ggplot2::geom_point)\n\ntopicExpr[[1]]\n## `::`\ntopicExpr[[2]]\n## ggplot2\ntopicExpr[[3]]\n## geom_point\n\nIf there is no colon operator designating the desired package, package is set to NULL on line 28.\n\nFinally, it’s time to actually access the documentation. First, the case of search being TRUE (??) is taken care of:\n\n\n    if (search) {\n        if (is.null(type)) \n            return(eval(substitute(help.search(TOPIC, package = PACKAGE), \n                list(TOPIC = as.character(topicExpr), PACKAGE = package))))\n        else return(eval(substitute(help.search(TOPIC, fields = FIELD, \n            package = PACKAGE), list(TOPIC = as.character(topicExpr), \n            FIELD = as.character(type), PACKAGE = package))))\n    }\n\n\nWe see that the function searching through the documentation is help.search() – the variables we have specified thusfar are provided as arguments and we’re done.\n\nNow we take care of the case where search is FALSE and type is NULL. Remember, type is NULL whenever the argument e2 is not supplied—  the most common use-case.\n\n\n    else {\n        if (is.null(type)) {\n            if (is.call(topicExpr)) \n                return(.helpForCall(topicExpr, parent.frame()))\n            topic <- if (is.name(topicExpr)) \n                as.character(topicExpr)\n            else e1\n            return(eval(substitute(help(TOPIC, package = PACKAGE), \n                list(TOPIC = topic, PACKAGE = package))))\n        }\n\n\nA few interesting things to note. First, we deal with the scenario where topicExpr is a call, in which case the function being used to access documentation is the unexported utils:::.helpforCall(). I haven’t dug through its body, but it looks like this is to allow users to execute code like ?sum() (instead of the more typical ?sum). But, I’ve noticed that it doesn’t work for everything—for an example run ?c() .\n\nStarting on line 41, we have the main way ? leads to documentation. This is how code like ?sum is evaluated, via a call to help() on line 44. Notice that substitute() is being used in a slightly different way than before, substituting the values in the “environment” defined on line 45 before eval() is run. (It is used the same way in the previous code chunk, on lines 31 and 33.)\n\nThe rest of the code is just dealing with the case where type and topic was specified by e1 and e2 arguments, respectively. It’s really just repeating what we’ve seen already, with the small addition of using utils:::.tryHelp() and utils:::topicName() functions.\n\n\n        else {\n            type <- if (is.name(type)) \n                as.character(type)\n            else e1\n            topic <- if (is.name(topicExpr)) \n                as.character(topicExpr)\n            else {\n                if (is.call(topicExpr) && identical(type, \"method\")) \n                  return(.helpForCall(topicExpr, parent.frame(), \n                    FALSE))\n                e2\n            }\n            if (type == \"package\") \n                package <- topic\n            h <- .tryHelp(topicName(type, topic), package = package)\n            if (is.null(h)) {\n                if (is.language(topicExpr)) \n                  topicExpr <- deparse(topicExpr)\n                stop(gettextf(\"no documentation of type %s and topic %s (or error in processing help)\", \n                  sQuote(type), sQuote(topicExpr)), domain = NA)\n            }\n            h\n        }\n    }\n}\n\n\n\nSo, what have we learned? ? is a convenience function wrapping around functions like help() and help.search(). It’s main purpose is to parse the different ways a user might refer to an object (e.g. ?ggplot2::geom_point, ??knn, ?sum(), ?sum).\n\n\n\nhelp()\nNow that we have a good grasp on what’s going on with ?, let’s see how help() works. Let’s start by echoing the body of the function:\n\nhelp\n\nfunction (topic, package = NULL, lib.loc = NULL, verbose = getOption(\"verbose\"), \n    try.all.packages = getOption(\"help.try.all.packages\"), help_type = getOption(\"help_type\")) \n{\n    types <- c(\"text\", \"html\", \"pdf\")\n    help_type <- if (!length(help_type)) \n        \"text\"\n    else match.arg(tolower(help_type), types)\n    if (!missing(package)) \n        if (is.name(y <- substitute(package))) \n            package <- as.character(y)\n    if (missing(topic)) {\n        if (!is.null(package)) {\n            if (interactive() && help_type == \"html\") {\n                port <- tools::startDynamicHelp(NA)\n                if (port <= 0L) \n                  return(library(help = package, lib.loc = lib.loc, \n                    character.only = TRUE))\n                browser <- if (.Platform$GUI == \"AQUA\") {\n                  get(\"aqua.browser\", envir = as.environment(\"tools:RGUI\"))\n                }\n                else getOption(\"browser\")\n                browseURL(paste0(\"http://127.0.0.1:\", port, \"/library/\", \n                  package, \"/html/00Index.html\"), browser)\n                return(invisible())\n            }\n            else return(library(help = package, lib.loc = lib.loc, \n                character.only = TRUE))\n        }\n        if (!is.null(lib.loc)) \n            return(library(lib.loc = lib.loc))\n        topic <- \"help\"\n        package <- \"utils\"\n        lib.loc <- .Library\n    }\n    ischar <- tryCatch(is.character(topic) && length(topic) == \n        1L, error = function(e) FALSE)\n    if (!ischar) {\n        reserved <- c(\"TRUE\", \"FALSE\", \"NULL\", \"Inf\", \"NaN\", \n            \"NA\", \"NA_integer_\", \"NA_real_\", \"NA_complex_\", \"NA_character_\")\n        stopic <- deparse1(substitute(topic))\n        if (!is.name(substitute(topic)) && !stopic %in% reserved) \n            stop(\"'topic' should be a name, length-one character vector or reserved word\")\n        topic <- stopic\n    }\n    paths <- index.search(topic, find.package(if (is.null(package)) \n        loadedNamespaces()\n    else package, lib.loc, verbose = verbose))\n    try.all.packages <- !length(paths) && is.logical(try.all.packages) && \n        !is.na(try.all.packages) && try.all.packages && is.null(package) && \n        is.null(lib.loc)\n    if (try.all.packages) {\n        for (lib in .libPaths()) {\n            packages <- .packages(TRUE, lib)\n            packages <- packages[is.na(match(packages, .packages()))]\n            paths <- c(paths, index.search(topic, file.path(lib, \n                packages)))\n        }\n        paths <- paths[nzchar(paths)]\n    }\n    structure(unique(paths), call = match.call(), topic = topic, \n        tried_all_packages = try.all.packages, type = help_type, \n        class = \"help_files_with_topic\")\n}\n<bytecode: 0x55cf7461ac50>\n<environment: namespace:utils>\n\n\nOf course, we’ll break this down into more digestible chunks.\n\nFirst, we are determining what format of documentation to get. There’s weird argument matching going on, but the main idea is that help can be served up in three forms: text, html, and pdf. By default, help() looks at the global option \"help_type\" for this.\n\n\nfunction (topic, package = NULL, lib.loc = NULL, verbose = getOption(\"verbose\"),\n  try.all.packages = getOption(\"help.try.all.packages\"), help_type = getOption(\"help_type\")) \n{\n    types <- c(\"text\", \"html\", \"pdf\")\n    help_type <- if (!length(help_type)) \n        \"text\"\n    else match.arg(tolower(help_type), types)\n\n\n\n\n\nNext, if the package argument is specified, we check that it is a name after it’s been quoted. If it is, the quoted argument is coerced into a string for later.\n\n\n    if (!missing(package)) \n        if (is.name(y <- substitute(package))) \n            package <- as.character(y)\n\n\n\nNow, we deal with the case where topic is not specified. This is not the typical case, topic is the first formal of help. For example, when you run help(geom_point) you’re setting topic = geom_point. However, this allows for things like help(package = ggplot2).\nThis is what’s going on in lines 13-27, help() is figuring out how to call library() correctly, given the user’s environment. It turns out that in addition to loading/attaching packages, when the help argument of library() is specified it returns information regarding the specified package (in an object of class \"packageInfo\").\n\n\n    if (missing(topic)) {\n        if (!is.null(package)) {\n            if (interactive() && help_type == \"html\") {\n                port <- tools::startDynamicHelp(NA)\n                if (port <= 0L) \n                  return(library(help = package, lib.loc = lib.loc, \n                    character.only = TRUE))\n                browser <- if (.Platform$GUI == \"AQUA\") {\n                  get(\"aqua.browser\", envir = as.environment(\"tools:RGUI\"))\n                }\n                else getOption(\"browser\")\n                browseURL(paste0(\"http://127.0.0.1:\", port, \"/library/\", \n                  package, \"/html/00Index.html\"), browser)\n                return(invisible())\n            }\n            else return(library(help = package, lib.loc = lib.loc, \n                character.only = TRUE))\n        }\n        if (!is.null(lib.loc)) \n            return(library(lib.loc = lib.loc))\n        topic <- \"help\"\n        package <- \"utils\"\n        lib.loc <- .Library\n    }\n\n\nNote, we’re making use of the lib.loc argument. It specifies the location of the R library trees on the user’s machine. By default, its value is NULL—this corresponds to the libraries according to .libPaths().\n\nNext is some simple cleaning-up of topic (which, at this point, we know was specified). Lines 35 and 36 are interesting, note the wrapping of the conditional in a TryCatch().\n\n\n    ischar <- tryCatch(is.character(topic) && length(topic) == \n        1L, error = function(e) FALSE)\n    if (!ischar) {\n        reserved <- c(\"TRUE\", \"FALSE\", \"NULL\", \"Inf\", \"NaN\", \n            \"NA\", \"NA_integer_\", \"NA_real_\", \"NA_complex_\", \"NA_character_\")\n        stopic <- deparse1(substitute(topic))\n        if (!is.name(substitute(topic)) && !stopic %in% reserved) \n            stop(\"'topic' should be a name, length-one character vector or reserved word\")\n        topic <- stopic\n    }\n\n\nAfter this chunk, we know that topic is a string of length 1.\n\nThe next step is to use the unexported function utils:::index.search() to search through relevant package for topic. This involves either searching through the entire set of packages in loadedNamespaces() or the specified package.\n\n\n    paths <- index.search(topic, find.package(if (is.null(package)) \n        loadedNamespaces()\n    else package, lib.loc, verbose = verbose))\n\n\nBelow, I’ve included (truncated) output from a few of these functions so that you can see what’s going on:\n\nloadedNamespaces()[1:4]\n## [1] \"grDevices\" \"digest\"    \"jsonlite\"  \"magrittr\"\n\nfind.package(loadedNamespaces())[1:4]\n## [1] \"/usr/lib/R/library/grDevices\"                           \n## [2] \"/usr/local/lib/R/site-library/digest\"                   \n## [3] \"/usr/local/lib/R/site-library/jsonlite\"                 \n## [4] \"/home/ubuntu/R/x86_64-pc-linux-gnu-library/4.2/magrittr\"\n\nutils:::index.search(\"anova\", find.package(loadedNamespaces()))\n## [1] \"/usr/lib/R/library/stats/help/anova\"\n\n\nNext, we’re cleaning up the try.all.packages argument. This sequence of logical operators works together to 1.) coerce try.all.packages into a logical and 2.) ensure try.all.packages is FALSE if at all possible (according to the documentation, if try.all.packages is TRUE there might be performance issues).\nBy default (and when it is called from ?), try.all.packages is FALSE, so this isn’t of much consequence.\n\n\n    try.all.packages <- !length(paths) && is.logical(try.all.packages) && \n        !is.na(try.all.packages) && try.all.packages && is.null(package) && \n        is.null(lib.loc)\n\n\n\nHere is where try.all.packages is used. If it is TRUE, an index.search() is performed for topic in every package in the.libPaths() directory with results being included in paths.\n\n\n    if (try.all.packages) {\n        for (lib in .libPaths()) {\n            packages <- .packages(TRUE, lib)\n            packages <- packages[is.na(match(packages, .packages()))]\n            paths <- c(paths, index.search(topic, file.path(lib, \n                packages)))\n        }\n        paths <- paths[nzchar(paths)]\n    }\n\n\n\nFinally, we have the end of help(). This is the code that fetches/loads the relevant documentation. If everything has gone correctly, R will try to find a way to show you the corresponding documentation. (If try.all.packages is TRUE, a search results page will be shown instead). In Rstudio, for example, the documentation file will appear in the “Help” pane.\n\n\n    structure(unique(paths), call = match.call(), topic = topic, \n        tried_all_packages = try.all.packages, type = help_type, \n        class = \"help_files_with_topic\")\n}\n\n\nWhy does this structure() call result in the documentation being displayed? I have no idea. I imagine it has something to do with the print method of the \"help_files_with_topic\" class— I haven’t been able to find great documentation on these details.\n\nTry it for yourself! Run the following code, it should bring up the documentation for stats::anova():\n\ntemp_pkgs <- find.package(\n  if (TRUE) loadedNamespaces() else \"stats\", \n  lib.loc = NULL, verbose = getOption(\"verbose\")\n)\n\ntemp_path <- utils:::index.search(\"anova\", temp_pkgs)\n\nstructure(temp_path, call = match.call(), topic = \"anova\", \n    tried_all_packages = FALSE, type = \"html\", \n    class = \"help_files_with_topic\")\n\n\nSo, what have we learned? Documentation for functions of loaded packages are accessed via a combination of the functions loadedNamespaces(), find.package(), and utils:::index.search(). If we want to access documentation of functions for packages that are not loaded, we need to use the functions .libPaths(), .packages(), and utils:::index.search()."
  },
  {
    "objectID": "posts/2022-02-15-Visualizing-BoardGameGeek-Data-with-ggdensity/Visualizing-BoardGameGeek-Data-with-ggdensity.html",
    "href": "posts/2022-02-15-Visualizing-BoardGameGeek-Data-with-ggdensity/Visualizing-BoardGameGeek-Data-with-ggdensity.html",
    "title": "Visualizing BoardGameGeek data with ggdensity",
    "section": "",
    "text": "In this blog post, we’re going to be looking at the BoardGameGeek data from week 4 of TidyTuesday 2022. This data set consists of community ratings and other stats for just over 20,000 board games. The first thing we need to do is load in the data and perform some basic cleaning, joining the ratings and details data on the id column:\n\nlibrary(\"tidyverse\")\n\ndata <- tidytuesdayR::tt_load('2022-01-25')\n\ndf <- data$ratings |>\n  left_join(data$details, by = \"id\")"
  },
  {
    "objectID": "posts/2022-02-15-Visualizing-BoardGameGeek-Data-with-ggdensity/Visualizing-BoardGameGeek-Data-with-ggdensity.html#looking-at-boardgamecategory",
    "href": "posts/2022-02-15-Visualizing-BoardGameGeek-Data-with-ggdensity/Visualizing-BoardGameGeek-Data-with-ggdensity.html#looking-at-boardgamecategory",
    "title": "Visualizing BoardGameGeek data with ggdensity",
    "section": "Looking at boardgamecategory",
    "text": "Looking at boardgamecategory\nSomething that immediately stands out to me is the variable boardgamecategory. Comparing stats across different types of board games could end up being really interesting! But, there is a problem—this column isn’t “tidy”:\n\nselect(df, name, boardgamecategory) |>\n  slice_head(n = 10)\n\n\n\n\n\n\n\n\n\nname\nboardgamecategory\n\n\n\n\nPandemic\n[‘Medical’]\n\n\nCarcassonne\n[‘City Building’, ‘Medieval’, ‘Territory Building’]\n\n\nCatan\n[‘Economic’, ‘Negotiation’]\n\n\n7 Wonders\n[‘Ancient’, ‘Card Game’, ‘City Building’, ‘Civilization’, ‘Economic’]\n\n\nDominion\n[‘Card Game’, ‘Medieval’]\n\n\nTicket to Ride\n[‘Trains’]\n\n\nCodenames\n[‘Card Game’, ‘Deduction’, ‘Party Game’, ‘Spies/Secret Agents’, ‘Word Game’]\n\n\nTerraforming Mars\n[‘Economic’, ‘Environmental’, ‘Industry / Manufacturing’, ‘Science Fiction’, ‘Space Exploration’, ‘Territory Building’]\n\n\n7 Wonders Duel\n[‘Ancient’, ‘Card Game’, ‘City Building’, ‘Civilization’, ‘Economic’]\n\n\nAgricola\n[‘Animals’, ‘Economic’, ‘Farming’]\n\n\n\n\n\n\nLuckily, this is an easy fix with some string processing. We can use stringr::str_extract_all() to extract the categories from each row into a list, then use tidyr::unnest() to flatten out the resulting list column.\n\ndf <- df |>\n  filter(!is.na(boardgamecategory)) |>\n  mutate(boardgamecategory = str_extract_all(boardgamecategory, \"(?<=')[^,]*(?=')\")) |>\n  unnest(boardgamecategory)\n\nselect(df, name, boardgamecategory) |>\n  slice_head(n = 10)\n\n\n\n\n\nname\nboardgamecategory\n\n\n\n\nPandemic\nMedical\n\n\nCarcassonne\nCity Building\n\n\nCarcassonne\nMedieval\n\n\nCarcassonne\nTerritory Building\n\n\nCatan\nEconomic\n\n\nCatan\nNegotiation\n\n\n7 Wonders\nAncient\n\n\n7 Wonders\nCard Game\n\n\n7 Wonders\nCity Building\n\n\n7 Wonders\nCivilization\n\n\n\n\n\n\nGreat! Now, let’s see what the most popular categories are:\n\ntop_categories <- df |>\n  group_by(boardgamecategory) |>\n  summarize(n = n()) |>\n  arrange(desc(n)) |>\n  slice_head(n = 10)\n\ntop_categories\n\n\n\n\n\nboardgamecategory\nn\n\n\n\n\nCard Game\n6402\n\n\nWargame\n3820\n\n\nFantasy\n2681\n\n\nParty Game\n1968\n\n\nDice\n1847\n\n\nScience Fiction\n1666\n\n\nFighting\n1658\n\n\nAbstract Strategy\n1545\n\n\nEconomic\n1503\n\n\nAnimals\n1354\n\n\n\n\n\n\nSurprisingly, the most popular board game category is “Card Games”! We can create a simple visual showing the prevalence of each of these top 10 categories:\n\n\nCode\ntop_categories |>\n  mutate(boardgamecategory = fct_reorder(boardgamecategory, n, .desc = TRUE)) |>\n  ggplot(aes(x = boardgamecategory, y = n)) +\n  geom_col() +\n  labs(\n    x = \"Category\",\n    y = NULL\n  )"
  },
  {
    "objectID": "posts/2022-02-15-Visualizing-BoardGameGeek-Data-with-ggdensity/Visualizing-BoardGameGeek-Data-with-ggdensity.html#looking-at-playingtime-minplayers-and-maxplayers",
    "href": "posts/2022-02-15-Visualizing-BoardGameGeek-Data-with-ggdensity/Visualizing-BoardGameGeek-Data-with-ggdensity.html#looking-at-playingtime-minplayers-and-maxplayers",
    "title": "Visualizing BoardGameGeek data with ggdensity",
    "section": "Looking at playingtime, minplayers, and maxplayers",
    "text": "Looking at playingtime, minplayers, and maxplayers\nLet’s put the work that we’ve done on the categories field on hold for a minute and look at how a game’s average number of players relates to its average play time. Before making any plots, I would suspect that as the number of players increases the average play time increases. That is to say, I would expect positive correlation between the two variables.\n\n\nCode\n# First, we need to do a little more cleaning\n# Filter out some outliers, compute avg_players\ndf <- df |> \n  filter(maxplayers < 20) |>\n  filter(playingtime < 1000) |> \n  mutate(playingtime = playingtime / 60) |>\n  mutate(avg_players = (minplayers + maxplayers)/2) \n  \ndf |>\n  distinct(name, .keep_all = TRUE) |> # Don't care about categories right now\n  ggplot(aes(x = avg_players, y = playingtime)) +\n  geom_jitter(height = .5, width = .5, size = .1, alpha = .5) +\n  scale_x_continuous(breaks = seq(0, 12, by = 2)) +\n  scale_y_continuous(breaks = seq(0, 14, by = 2)) +\n  coord_cartesian(ylim = c(0, 14), expand = FALSE) +\n  labs(\n    x = \"Average no. of players\",\n    y = \"Average play time (Hours)\"\n  )\n\n\n\n\n\nInterestingly, this does not seem to be the case! In fact it seems like it may be the opposite—play time appears to be maximized when there are between 2 and 4 players and drops off as the number of players increases.\nUnfortunately, the above plot has a few issues that stand in the way of us making useful observations. First, I have had to do some severe jittering to eliminate graphical artifacts resulting from the discrete nature of the data. Notice, several of the points seem to correspond to games with fewer than 0 average players! Second, there is pretty severe overplotting. Although I have attempted to avoid this by setting both the size and alpha arguments, the plot is still very crowded—especially around the horizontal axis between the 2 and 4 player ticks.\nFortunately, I know of a tool that can help with both of these issues—ggdensity!\n\n\nCode\nlibrary(\"ggdensity\")\n\ndf |> \n  distinct(name, .keep_all = TRUE) |> # Don't care about categories right now\n  ggplot(aes(x = avg_players, y = playingtime)) +\n  geom_hdr(adjust = c(2, 4)) + # Need to set adjust b/c of discreteness\n  scale_x_continuous(breaks = seq(0, 12, by = 2)) +\n  scale_y_continuous(breaks = seq(0, 14, by = 2)) +\n  coord_cartesian(ylim = c(0, 14), expand = FALSE) +\n  labs(\n    x = \"Average no. of players\",\n    y = \"Average play time (Hours)\"\n  )\n\n\n\n\n\nAbove, we are plotting estimated “Highest Density Regions” (HDRs)— these are the smallest regions containing 50%, 80%, 95%, and 99% of the data (essentially). For more information, check out the ggdensity repo. See that the issues of overplotting and jittering are eliminated as we have abandoned the strategy of plotting individual points. Now that we’ve taken care of these problems, we can see that there is a negative association between the average number of players and average play time. This is unexpected! Let’s look a little deeper, leveraging our previous work on boardgamecategory."
  },
  {
    "objectID": "posts/2022-02-15-Visualizing-BoardGameGeek-Data-with-ggdensity/Visualizing-BoardGameGeek-Data-with-ggdensity.html#putting-it-all-together",
    "href": "posts/2022-02-15-Visualizing-BoardGameGeek-Data-with-ggdensity/Visualizing-BoardGameGeek-Data-with-ggdensity.html#putting-it-all-together",
    "title": "Visualizing BoardGameGeek data with ggdensity",
    "section": "Putting it all together",
    "text": "Putting it all together\nDoes this negative association hold true across the most popular categories? Or is this yet another example of Simpson’s paradox? There’s only one way to find out—faceting!\n\n\nCode\ndf |> \n  filter(boardgamecategory %in% top_categories$boardgamecategory[1:5]) |>\n  mutate(boardgamecategory = fct_reorder(boardgamecategory, playingtime, mean, .desc = TRUE)) |>\n  ggplot(aes(x = avg_players, y = playingtime, fill = boardgamecategory)) +\n  geom_hdr(adjust = 2) + # Need to set adjust b/c of discreteness\n  facet_wrap(vars(boardgamecategory), ncol = 5) +\n  scale_x_continuous(breaks = seq(0, 12, by = 2)) +\n  scale_y_continuous(breaks = seq(0, 14, by = 2)) +\n  scale_fill_brewer(type = \"qual\", palette = 2, guide = NULL) +\n  coord_cartesian(ylim = c(0, 14), expand = FALSE) +\n  labs(\n    x = \"Average no. of players\",\n    y = \"Average play time (Hours)\"\n  )\n\n\n\n\n\nThis plot offers a new perspective. In each category, it appears to be the case that average play time and number of players are independent. Also, it looks like “Wargame” board games tend to involve fewer players and last signficantly longer than other categories. Combining board games across categories creates the illusion that play time and number of players is negatively correlated—another point for Simpson!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Sentiment towards The Rings of Power\n\n\nComparing the responses of book readers and non-book readers\n\n\n\n\n\n\nOct 15, 2022\n\n\nJames Otto\n\n\n\n\n\n\n  \n\n\n\n\nVisualizing BoardGameGeek data with ggdensity\n\n\n\n\n\n\n\n\n\nFeb 15, 2022\n\n\nJames Otto\n\n\n\n\n\n\n  \n\n\n\n\nUnderstanding base documentation functions\n\n\n\n\n\n\n\n\n\nJan 24, 2022\n\n\nJames Otto\n\n\n\n\n\n\nNo matching items"
  }
]